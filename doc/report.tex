\documentclass{article} % For LaTeX2e
\usepackage{nips, times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{relsize}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{paralist}

\input math.tex

\nipsfinalcopy

\begin{document}
\title{Compressed Sensing for Traffic Networks}

\author{
Cathy Wu\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{cathywu@eecs.berkeley.edu}\\
\And
Philipp Moritz\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{pcm@eecs.berkeley.edu}\\
\And
Richard Shin\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{ricshin@eecs.berkeley.edu}\\
\And
Fanny Yang\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{fanny-yang@eecs.berkeley.edu} \\
}

\maketitle

\begin{abstract}
Reconstructing true parameters from a partially observed model is a fundamental problem in science and technology.
In the last years there has been a surge of interest in this task under a broad variety of model assumptions.
Motivated by applications in traffic reconstruction, we investigate when blocks of probability distributions can be recovered from linear measurements.
We empirically study the performance of several reconstruction methods and review the literature on provably correct methods for the reconstruction of probability measures.
We evaluate several possible regularizers that can be used if the usual $\ell^1$ reconstruction is not applicable, because probability distributions have a built in $\ell^1$ constraint.
\end{abstract}

\section{Introduction}
\paragraph{Related work.}
\section{Problem formulation}
\begin{equation}\label{l0-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_0\\
    \text{subject to} &\quad Ax = f\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}

\begin{equation}\label{l1-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_1\\
    \text{subject to} &\quad Ax = f\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}

\begin{equation}\label{l8-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_\infty\\
    \text{subject to} &\quad Ax = f\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}
\section{Guarantees for sparse recovery}
To guarantee exact sparse recovery, the following procedure will be used: First, we make sure that the original vector~$x^*$ is actually the sparsest solution to~\eqref{l0-minimization}. Then, we prove that the solution of the relaxed problems~\eqref{l1-minimization} and~\eqref{l8-minimization} are unique and equal to $x^*$.
\subsection{Sparse representation and approximation results}
To show that the original vector is actually the unique sparsest one, we use the following theorem from
\begin{theorem}
\label{thm:l0unique}
Let $A \in \R^{m\times n}$ and $x\in\R^$ be $s$-sparse. In the case
$m\geq 2k$, the problem $Ax = b$ can be recovered if and only if
\begin{inparaenum}[(a)]
\item the set $\Sigma_{2k}$ of $2k$-sparse vectors intersects with the null space of the matrix $\Phi$ only at the singleton $\{0\}$ (see also section 2 of \cite{Dahmen_CS}).
\item For any subset $S$ with $|S| = 2k$, the matrix $\rank \Phi_S = 2k$ and
\item $\Phi_S^T \Phi_S$ is invertible.
\end{inparaenum}

\subsection{Obtaining the sparse solution by Compressed Sensing}
\subsection{Recovery of sparse probability measures}
\subsection{Recovery of sparse block probability measures}
\section{Algorithms for reconstructing block probability measures}
\section{Numerical Results}
\section{References}

\FloatBarrier
\vskip 0.2in
\bibliographystyle{plainnat}
\bibliography{lit}
\end{document}
