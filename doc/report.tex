\documentclass{article} % For LaTeX2e
\usepackage{nips, times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{relsize}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{paralist}

\input math.tex

\nipsfinalcopy

\begin{document}
\title{Compressed Sensing for Traffic Networks}

\author{
Cathy Wu\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{cathywu@eecs.berkeley.edu}\\
\And
Philipp Moritz\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{pcm@eecs.berkeley.edu}\\
\And
Richard Shin\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{ricshin@eecs.berkeley.edu}\\
\And
Fanny Yang\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{fanny-yang@eecs.berkeley.edu} \\
}

\maketitle

\begin{abstract}
Reconstructing true parameters from a partially observed model is a fundamental problem in science and technology.
In the last years there has been a surge of interest in this task under a broad variety of model assumptions.
Motivated by applications in traffic reconstruction, we investigate when blocks of probability distributions can be recovered from linear measurements.
We empirically study the performance of several reconstruction methods and review the literature on provably correct methods for the reconstruction of probability measures.
We evaluate several possible regularizers that can be used if the usual $\ell^1$ reconstruction is not applicable, because probability distributions have a built in $\ell^1$ constraint.
\end{abstract}

\section{Introduction}
\paragraph{Related work.}
\section{Problem formulation}
\begin{equation}\label{l0-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_0\\
    \text{subject to} &\quad Ax = f\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}

\begin{equation}\label{l1-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_1\\
    \text{subject to} &\quad Ax = f\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}

\begin{equation}\label{l8-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_\infty\\
    \text{subject to} &\quad Ax = f\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}
\section{Guarantees for sparse recovery}
To guarantee exact sparse recovery, the following procedure will be used: First, we make sure that the original vector~$x^*$ is actually the sparsest solution to~\eqref{l0-minimization}. Then, we prove that the solution of the relaxed problems~\eqref{l1-minimization} and~\eqref{l8-minimization} are unique and equal to $x^*$.
\subsection{Sparse representation and approximation results}
To show that the original vector is actually the unique sparsest one, we use the following theorem from
\begin{theorem}
\label{thm:l0unique}
Let $A \in \R^{m\times n}$ and $x\in\R^n$ be $s$-sparse. In the case
$m\geq 2k$, the problem $Ax = b$ can be recovered if and only if
\begin{inparaenum}[(a)]
\item the set $\Sigma_{2k}$ of $2k$-sparse vectors intersects with the null space of the matrix $\Phi$ only at the singleton $\{0\}$ (see also section 2 of \cite{Dahmen_CS}).
\item For any subset $S$ with $|S| = 2k$, the matrix $rank \Phi_S = 2k$ and
\item $\Phi_S^T \Phi_S$ is invertible.
\end{inparaenum}
\end{theorem}

\subsection{Obtaining the sparse solution by Compressed Sensing}
\subsection{Recovery of sparse probability measures}
\subsection{Recovery of sparse block probability measures}
\section{Algorithms for reconstructing block probability measures}
\subsection{Entropy minimization}
For a probability distribution, entropy is defined as
\begin{equation*} H(X) = E[-\ln P(X)] = \sum_{i=1}^{n} -p_i \ln p_i \end{equation*}
where $p_i$ is probability of discrete event $i$.
Intuitively, a random variable with a sparse probability measure has a more unpredictable value, and therefore lower entropy (in particular, if the random variable has only one possible value, meaning that the probability of one event is 1 and the rest are 0, then the entropy is minimized at 0).
In addition, entropy provides a lower bound on the cardinality of a probability distribution. When $\|x\|_0 = n$, then maximum entropy is attained when the $n$ non-zero possibilities have equal weight. $\max H(x) = -\sum \frac{1}{n} \log \frac{1}{n} = -\log \frac{1}{n} = \log n$; therefore, $H(x) < \|x\|_0$.

We provide the following argument for why it may be preferable to optimize for entropy instead of the $\ell_\infty$ norm.
Consider the probability distribution $x = [0.5, 0.5, 0]$ over three discrete possibilities.
If we use $\frac{1}{\max x_i}$ as the criterion to optimize, then all distributions where one of the entries remains $0.5$ are equally preferred.
With entropy, as we increase the $x_3 = 0$, one of $x_1$ and $x_2$ must increase correspondingly; for example, $x' = [0.7729, 0.1135, 0.1135]$ has the same entropy as $x$.
This seems to better fit our assumption that most people from each origin will go to a small number of destinations, which is why we desired sparsity in the first place.

In entropy minimization, we attempt to solve the following:
\begin{align*}
&  \min_{x} \sum_\gamma H(x_\gamma) \qquad \text{ s.t. } Ax = b, \sum_i x_{\gamma, i} = 1\ \forall \gamma \\
=& \min_{x} \gamma H(x) \qquad \text{ s.t. } Ax = b, \sum_i x_{\gamma, i} = 1\ \forall \gamma
\end{align*}
%This problem also allows us to distinguish between multiple solutions which have the same cardinality, since two solutions which are not exactly the same will have different entropy.
Unfortunately, $H(X)$ is a concave function, so the above is a concave minimization problem, albeit with a convex set as the constraint.
However, as $H(X)$ is differentiable, we can still use a first-order gradient descent method for constrained problems (without any guarantees on optimality) in the following way:
\begin{itemize}
  \item Find a feasible $x$, i.e., which satisfies $Ax = b, \sum_i x_{\gamma, i} = 1\ \forall \gamma$.
  \item Compute the gradient of the objective function at $x_0$.
  \item Find a feasible update direction $\Delta x$, selected to minimize the dot product between it and the gradient.
  \item Find $\alpha \le 1$ so that $H(x + \alpha \Delta x) < H(x)$. If no such $\alpha$ exists, terminate.
  \item Update $x_\textrm{new} = x + \alpha \Delta x$, and repeat the above steps.
\end{itemize}
\cite{sudoku} describes a similar method for entropy minimization, but they take additional steps to try to escape local minima, by occasionally updating $x$ to increase entropy instead of decreasing it.

\section{Numerical Results}
\section{References}

\FloatBarrier
\vskip 0.2in
\bibliographystyle{plainnat}
\bibliography{lit}
\end{document}
