\documentclass{article} % For LaTeX2e
\usepackage{nips, times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{relsize}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{paralist}

\renewcommand{\null}{\operatorname{null}}
\newcommand{\given}{\,|\,}

\input math.tex

\nipsfinalcopy

\begin{document}
\title{Compressed Sensing for Traffic Networks}

\author{
Philipp Moritz\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{pcm@eecs.berkeley.edu}\\
\And
Richard Shin\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{ricshin@eecs.berkeley.edu}\\
\And
Cathy Wu\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{cathywu@eecs.berkeley.edu}\\
\And
Fanny Yang\\
University of California\\
Berkeley, CA 94720, USA\\
\texttt{fanny-yang@eecs.berkeley.edu} \\
}

\maketitle

\begin{abstract}
Reconstructing true parameters from a partially observed model is a fundamental problem in science and technology.
In the last years there has been a surge of interest in this task under a broad variety of model assumptions.
Motivated by applications in traffic reconstruction, we investigate when blocks of probability distributions can be recovered from linear measurements.
We empirically study the performance of several reconstruction methods and review the literature on provably correct methods for the reconstruction of probability measures.
We evaluate several possible regularizers that can be used if the usual $\ell^1$ reconstruction is not applicable, because probability distributions have a built in $\ell^1$ constraint.
\end{abstract}

\section{Introduction}
\paragraph{Related work.}
\section{Problem formulation}
\begin{equation}\label{l0-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_0\\
    \text{subject to} &\quad Ax = b\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}

\begin{equation}\label{l1-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_1\\
    \text{subject to} &\quad Ax = b\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}

\begin{equation}\label{l8-minimization}
  \begin{split}
    \text{minimize} &\quad \norm{x}_\infty\\
    \text{subject to} &\quad Ax = b\quad\text{and}\quad 1^\T x = 1
  \end{split}
\end{equation}
\section{Guarantees for sparse recovery}
To guarantee exact sparse recovery, the following procedure will be used: First, we make sure that the original vector~$x^*$ is actually the sparsest solution to~\eqref{l0-minimization}. Then, we prove that the solution of the relaxed problems~\eqref{l1-minimization} and~\eqref{l8-minimization} are unique and equal to $x^*$.
\subsection{Sparse representation results}
To show that the original vector is actually the unique sparsest one, we use the following theorem from \cite{Dahmen_CS}.
\begin{theorem}
\label{thm:l0unique}
Let $A \in \R^{m\times n}$ and $x\in\R^n$ be $s$-sparse. In the case
$m\geq 2k$, the problem $Ax = b$ can be recovered if and only if
\begin{inparaenum}[(a)]
\item the set $\Sigma_{2k}$ of $2k$-sparse vectors intersects with the null space of the matrix $A$ only at the singleton $\{0\}$ (see also section 2 of \cite{Dahmen_CS}).
\item For any subset $S$ with $|S| = 2k$, the matrix $A_S$ has rank $2k$ and
\item $A_S^\T A_S$ is invertible.
\end{inparaenum}
\end{theorem}

\subsection{Obtaining the sparse solution by Compressed Sensing}
The convex relaxation of the original $\ell_0$-minimization problem
has been studied in many different settings by the Compressed Sensing
community.
The following elementary but particularly elegant (although not optimal) example was given by Benjamin Recht during the ``BigData Bootcamp'' at the Simons Institute in Berkeley.
\begin{theorem}
  Let $x^*\in \R^n$ be $s$-sparse and $A \in \R^{m\times n}$ be a
  random matrix with independent entries $A_{ij}\sim \text{N}(0,
  m^{-1})$. If $m \geq 2 (1+\epsilon) s\log n + s + 1$ then $x^*$ is
  the unique solution of
  \begin{align}
    &\text{minimize}\quad \norm{x}_1\label{CS}\\
    &\text{subject to}\quad Ax = Ax^* = b \nonumber
  \end{align}
  with probability at least $1 - 2\exp\bigl(-\bigl(\sqrt{(1+\epsilon)/(2s) +
      \epsilon} - \sqrt{(1+\epsilon)/(2s)}\bigr)^2\cdot \log n\bigr).$
\end{theorem}

With a stronger albeit less elegant technique than the one Ben used, the factor $\log n$ in the bound on the number of constraints can be improved to $\log(n/s)$.
%% TODO: Uniqueness
\begin{proof}
Let $S$ denote the support of $x^*$. The proof relies on the so called \emph{primal-dual witness} technique \cite{Wainwright2009}: We construct a pair of vectors that are primal and dual optimal and act as a witness for correct support recovery.
The following lemma formalizes the notion of a primal-dual witness $(x, \lambda)\in \R^{n\times m}$.
\begin{lemma}\label{l1-lemma}
  (a) A vector $x \in \R^n$ is optimal for \eqref{CS} if there exists
  a vector $\lambda\in \R^m$ such that $A^\T \lambda\in \partial
  \norm{x}_1$ and $Ax = b$ and (b) if the vector $v = A^\T \lambda$
  satisfies the strict dual feasibility condition $|v_j| < 1$
  for all $j\not\in S$, any optimal solution to \eqref{CS} satisfies
  $x_j = 0$ for all $j\not\in S$.
\end{lemma}
The proof is immediate: The problem \eqref{CS} is solved by $x\in
\R^n$ if and only if $\norm{x + h}_1 \geq \norm{x}_1$ for all $h\in \null(A)$.
The latter condition is true if there is a $\lambda \in \R^m$ such
that $A^\T \lambda \in \partial \norm{x}_1$ and $Ax = b$, because from
the subgradient condition we can then for all $h\in \null(A)$ conclude
\begin{equation*}
\norm{x + h}_1 \geq \norm{x}_1 + \sprod{A^\T \lambda, h} = \norm{x}_1 +
  \sprod{\lambda, A h} = \norm{x}_1.
\end{equation*}
The second part immediately follows from $v \in \partial\norm{x}_1$,
thus $v_j \in \{+1, -1\}$ if $x_j \neq 0$. \qedhere

Back to the main proof, we thus have to construct a Lagrange multiplier $\lambda$ such that $v = A^\T \lambda$ and $v_k = \sign(x^*_k)$ if $x^*_k\neq 0$ and $|v_k| < 1$ otherwise. Let $S$ be the support of $x^*$, then we define $e = \sign(x^*_S)\in \R^s$. Our task is then to find $\lambda$ such that
\begin{align}\label{compressed_equation}
  (A^\T \lambda)_S = e\quad\text{and}\quad \norm{(A^\T \lambda)_{S^c}}_\infty < 1.
\end{align}
  We can assume that $A = (A_S, A_{S^c})$ and $x = (x_S, x_{S^c})^\T$. The strategy is to solve the first equation in~\eqref{compressed_equation} by least squares and then verify the second equation. The optimization problem that has to be solved is
  \begin{align*}
    &\text{minimize}\quad \norm{\lambda}^2\\
    &\text{subject to}\quad A_S^\T \lambda = e.
  \end{align*}
    Because $A$ is Gaussian and thus of full rank\footnote{this is in fact in general true for matrices with indices drawn i.i.d. from a continuous distribution}, the solution can be calculated using the Moore-Penrose pseudoinverse $\lambda = A_S(A_S^\T A_S)^{-1} e$.
It remains to show that for $z = A_{S^c}^\T\lambda$ the $l^{\infty}$ norm is strictly smaller than $1$ with high probability. First observe that $\|\lambda\|^2 = e^\T(A_S^\T A_S)^{-1} e$.
From this it follows\footnote{if $x_k \sim \operatorname{N}_p(0, \Sigma)$ is from a multivariate Gaussian and $X = (x_1, \dots, x_n)^\T \in \R^{n\times p}$, then $(X^\T X)^{-1} \sim W_p^{-1}(\Sigma^{-1}, n)$ is from an inverse Wishart distribution and therefore $z^\T (X^\T X)^{-1} z$ is an inverse $\chi^2$ variable with parameters $z^\T \Sigma^{-1} z/2$ and $(n-p+1)/2$.} that $\|\lambda\|^2\sim \text{Inv-$\chi^2$}((m-s+1)/2, m \norm{e}^2/2)$ where $\norm{e}^2 = s$. With the large deviation bound for the inverse $\chi^2$
\begin{align*}
  \P\left(\norm{\lambda} > \sqrt{\frac{ms}{m-s+1-t}}\right) \leq
  \exp\left(-\frac{t^2}{4(m-s+1)}\right)
\end{align*}
we get $\P(|z_k| > 1 \given \norm{\lambda} \leq
\sqrt{ms/(m-s+1-t)}) \leq \P(|w| > 1\given w\sim N(0, s/(m-s+1-t))
\leq \exp(-(m-s+1-t)/(2s))$ and therefore we have
\begin{equation*}
  \P(\norm{z}_\infty > 1) \leq (n-s)\cdot \exp\left(-\frac{m-s+1-t}{2s}\right) + 
  \exp\left(-\frac{t^2}{4(m-s+1)}\right).
\end{equation*}
Setting $t = 2(1+\epsilon) \cdot s\cdot \log n
\cdot (\sqrt{1-2s\epsilon/(1+\epsilon)} - 1)$ concludes the proof.\qed
    \end{proof}

\subsection{Recovery of sparse probability measures}
We will present two methods for the reconstruction of sparse probability measures:  Regularization with the $\ell^\infty$ norm and minimal cardinality selection.
\paragraph{Regularization with the $\ell^\infty$ norm.} If the vector $x^*$ to be recovered is a sparse probability measure, the technique described in the previous section is not applicable, because $\norm{x}_1 = 1$ for probability measures.
Unless the $k$-sparse probability measure $x^*$ is the unique solution of \eqref{l1-minimization}, we thus have to come up with a different regularization scheme to achieve strong recovery results.
In \cite{mert} it was observed that for probability measures $x$ we have $1 = \norm{x}_1 \leq \norm{x}_0\cdot \norm{x}_\infty$ and thus $\norm{x}_{\infty}^{-1}$ is a lower bound on the size of the support of $x$.
This suggests trying to use the inverse $\ell^\infty$ norm as a proxy for sparsity.

Let $p^*$ denote the minimum value of $\norm{x}_0$ over $Ax = b$, we then consider the relaxation
\begin{equation}\label{l8-recovery}
  \begin{array}{rcllllr}
  p^* \geq& p_\infty^* &= \min\limits_x\, \norm{x}_\infty^{-1}&\text{such that $Ax = b$, $1^\T x = 1$}\quad \text{and}\\[5pt]
  &1/p_\infty^* &= \max\limits_{i=1,\dots,n} \max\limits_{x}\,x_i&\text{such that $Ax = b$, $1^\T x = 1$.}
  \end{array}
\end{equation}

\paragraph{Minimal cardinality selection.} Instead of taking the maximum over $1\leq i\leq n$ in the linear program formulation of \eqref{l8-recovery}

%% Vandermonde matrix: if min card(x) unique, then recovery, not for L1

%% Boyd, Candes: reweighted L1, gives the card-approximation

%% Sahan, Negaban + martin: unified framework for M-estimators

%% \cite{foygel}

\subsection{Recovery of sparse block probability measures}
%% TODO Fanny: Theory and definitions go here
We build on the guarantees given in \cite{inverse_problems} for the reconstruction of Gaussian measurements.
\begin{theorem}
  Let $A\in \R^{m\times n}$ be a random matrix with independent
  entries $A_{ij}\sim N(0, m^{-1})$ and let $\Omega = T_{\mathcal
    A}(x^*)\cap \mathbb{S}^{n-1}$ denote the spherical part of the
  tangent cone $T_{\mathcal A}(x^*)$. Suppose that we have
  measurements $b = Ax^*$. Then $x^*$ is the unique optimum of
  \eqref{l1-minimization} with probability at least $1 - \exp(-\frac 1
  2 (\lambda_n - w(\Omega))^2$ if we have $m\geq w(\Omega)^2 + 1$
  constraints.
\end{theorem}
\begin{proof}
  This is Corollary 3.3 from \cite{inverse_problems}.
\end{proof}

\begin{theorem}
  If we want to recover a block structured probability measure, we
  need a certain number of measurements for reconstruction with
  certain probability.
\end{theorem}
\begin{proof}
  The Gaussian width of $\Omega$ can be calculated as
  \begin{align*}
    w(\Omega) = \E_g\left(\sup_{z\in \Omega}\,\sprod{g, z}\right) \leq
    \E_g\left(\sup_{z\in\Omega}\,\norm{g}_{\infty}\norm{z}_1\right)
  \end{align*}
  Let $S$ be the support set of $x^*$ and $z = \hat x - x^*$, then we
  have $z_{S^c} = \hat x_{S^c} \geq 0$ and $1^\T z = 0$ and thus
\end{proof}

The second factor $\E_g[\norm{g}_\infty]$ can be bounded in the following way.
For $g\sim N(0, 1)$ we have $\P(|g|>t) \leq 2e^{-t^2/2}$ and therefore $\P(\max_{1\leq i\leq n} |g_i| > t) \leq 2ne^{-t^2/2}$. Therefore, we get
\begin{equation*}
  \E_g[\norm{g}_\infty] \leq x + 2n\int_{t=x}^\infty e^{-t^2/2}\,\d t
  \leq x + 2n\cdot \frac 1 x \cdot e^{-x^2/2}.
\end{equation*}
Choosing $x = \sqrt{2\log n}$ yields $\E_g[\norm{g}_\infty] \leq
\sqrt{2\log(n)} + 2$.

This reconstruction procedure may seem suboptimal, because it does not
take into account the block structure of our measurements.
However,
knowledge of this structure does not help much in our situation as we
argue now. The number of measurements needed for reconstruction in
compressed sensing is typically of the order
$\O(k\log(n/k))$.
Intuitively, the first factor specifies the number
of measurements necessary to recover the $k$ nonzero entries of the
true parameter and the second factor is the number of measurements
needed for estimating the right support. If we have constraints on the
support, we can use a technique like \emph{model-based compressed
  sensing} \cite{model_sensing} to reduce this second factor.  
Let $m_k$ be the number of possible support configurations with
sparsity $k$, then with a model-based approach we need on the order or
$\O(k\log(m_k))$ measurements.
For $m_k = {n\choose k}\approx (ne/k)^k$, we recover the compressed
sensing result.
In our case, if we have $b$ blocks of size $\nu$ each, each with
sparsity $\kappa$ (thus $n = b\nu$ and $k=b\kappa$), we get
\begin{equation*}
m_k = {\nu \choose \kappa}^b \approx \left(\frac{\nu
  e}{\kappa}\right)^{b\kappa} = \left(\frac{ne}{k}\right)^k,
\end{equation*}
which is the same result as above. The interpretation of this is that
the support configurations that violate the block assumption are a
negligible portion of all the possible support configurations.


\section{Algorithms for reconstructing block probability measures}
\subsection{Entropy minimization}
For a discrete random variable $X$ with values in $\{1, \dots, n\}$ and probabilities $\P(X = k) = p_k$, the entropy is defined as $H(X)  = -\sum_{i=1}^{n} p_i \log p_i$.
Intuitively, a random variable with a sparse probability measure has a more unpredictable value, and therefore lower entropy (in particular, if the random variable has only one possible outcome, then the entropy is 0).
In addition, entropy provides a lower bound on the cardinality of a probability distribution. When $\|x\|_0 = n$, then maximum entropy $H(x) = \log n$ is attained when the $n$ non-zero possibilities have equal weight and therefore $H(x) < \|x\|_0$.

We provide the following argument for why it may be preferable to optimize for entropy instead of the $\ell_\infty$ norm.
Consider the probability distribution $p = (0.5, 0.5, 0)$ over three discrete possibilities.
If we use $\norm{p}_{\infty}^{-1}$ as the criterion to optimize, then all distributions where one of the entries remains $0.5$ are equally preferred.
With entropy, as we increase $p_3$, one of $p_1$ or $p_2$ must increase correspondingly; for example, $q = (0.7729, 0.1135, 0.1135)$ has the same entropy as $p$.
This seems to better fit our assumption that most people from each origin will go to a small number of destinations, which is why we desired sparsity in the first place.

In entropy minimization, we attempt to solve the following:
\begin{align*}
&  \min_{x} \sum_\gamma H(x_\gamma) \qquad \text{ s.t. } Ax = b, \sum_i x_{\gamma, i} = 1\ \forall \gamma \\
=& \min_{x} \gamma H(x) \qquad \text{ s.t. } Ax = b, \sum_i x_{\gamma, i} = 1\ \forall \gamma
\end{align*}
%This problem also allows us to distinguish between multiple solutions which have the same cardinality, since two solutions which are not exactly the same will have different entropy.
Unfortunately, $H(X)$ is a concave function, so the above is a concave minimization problem, albeit with a convex set as the constraint.
However, as $H(X)$ is differentiable, we can still use a first-order gradient descent method for constrained problems (without any guarantees on optimality) in the following way:
\begin{itemize}
  \item Find a feasible $x$, i.e., which satisfies $Ax = b, \sum_i x_{\gamma, i} = 1\ \forall \gamma$.
  \item Compute the gradient of the objective function at $x_0$.
  \item Find a feasible update direction $\Delta x$, selected to minimize the dot product between it and the gradient.
  \item Find $\alpha \le 1$ so that $H(x + \alpha \Delta x) < H(x)$. If no such $\alpha$ exists, terminate.
  \item Update $x_\textrm{new} = x + \alpha \Delta x$, and repeat the above steps.
\end{itemize}
\cite{sudoku} describes a similar method for entropy minimization, but they take additional steps to try to escape local minima, by occasionally updating $x$ to increase entropy instead of decreasing it.

\section{Numerical Results}
\section{References}

\FloatBarrier
\vskip 0.2in
\nocite{*}
\bibliographystyle{plainnat}
\bibliography{lit}
\end{document}
